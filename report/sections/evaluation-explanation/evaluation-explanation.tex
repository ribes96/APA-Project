\section{Metric used to evaluate the models}
To evaluate our models we chose the F2 score, which is the harmonic mean of recall and precision with more emphasis on precision. We chose this score because the two result classes of our dataset are quite imbalanced. Because we are especially interested in a low number of false negatives, i.e. predictions that a patient will not die within a year when he actually does, we defined precision and recall as follows:

\begin{align*}
precision &= \frac{True Negative}{True Negative + False Negative} \\
recall &= \frac{True Negative}{True Negative + False Positive}
\end{align*}

The F2 score is then defined as:
\begin{align*}
F2 = (1 + 2^2) * \frac{precision * recall}{2^2 * precision + recall}
\end{align*}

One advantage of this score is that we can put more weight to the underrepresented class. At the same time the F2 score has the disadvantage that it does not give a clear result such as for example the accuracy score because the F2 score consists of the combination of two different values leading to possibly similar F2 scores with different prediction results. Therefore we will not only trust blindly in the calculated F2 score but will consider the confusion matrices of each model as well to determine the best possible model.