% 5. The results obtained using
%  at least three linear/quadratic methods (indicating
% the best set of parameters for each one):
% (a) If the task is classifcation, any of:
%  logistic regression, multinomial regression
% (single-layer MLP), LDA, QDA, RDA, Naive Bayes, nearest-neighbours, linear
% SVM, quadratic SVM
% (b) If the task is
%  regression, any of:
%  linear regression, ridge regression, the LASSO,
% nearest-neighbours, linear SVM, quadratic SVM
%  at least two general non-linear
%
\section{Results obtained using linear/quadratic methods}


\subsection{Naive Bayes}

The Naive Bayes Algorithm has the advantage that it does not distinguish between types of data and it does not perform any implicit transformation. The typical disadvantadge of this method is that it assumes independence of the attributes of the data.

As we have mixed data, the advantage is very appropriate. Looking at our data, it seems like most of the attributes are independent, so we can assume the results will be good. In fact, as we are working with a reduced dataset (in which we have removed some of the attributes), we could expect that the results will be even better in that one.

As this is a quite simple method, it does not need parameters, so we do not have to chose any hyper-parameter.

The results obtained using this model are:



\begin{table}[!htbp]
\centering
\caption{NB Original}
\vspace{0.1cm}
\label{nb-orig}
\begin{tabular}{|l|ll|ll}
\cline{1-3}
Prediction & \multicolumn{2}{l|}{Reference} &  &  \\
           & False          & True          &  &  \\ \cline{2-3}
False      & 32              & 3             &  &  \\
True       & 93              & 21             &  &  \\ \cline{1-3}
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{NB Removed}
\vspace{0.1cm}
\label{nb-remov}
\begin{tabular}{|l|ll|ll}
\cline{1-3}
Prediction & \multicolumn{2}{l|}{Reference} &  &  \\
           & False          & True          &  &  \\ \cline{2-3}
False      & 122              & 18             &  &  \\
True       & 5              & 4             &  &  \\ \cline{1-3}
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{F2 on NB}
\vspace{0.1cm}
\label{nb-f2}
\begin{tabular}{|l|l|l|}
\hline
      & Original & Removed \\ \hline
Naive Bayes & 0.2990654  & 0.9413580       \\ \hline
\end{tabular}
\end{table}

% \red{Mosrar los resultados y la confussion matrix}

At the first sight, it stands out the fact that the results from the ``original'' dataset and the ``removed'' one are very different . While ``original'' only shows a F2 score about $0.3$, the other one shows much better results with a F2 score of $0.94$, as expected. This leads us to think that the attributes we removed weren't indeed independent from the others, and keeping theme out has helped the model.\\
But also we can see that the model predicts the DIED = TRUE cases quite well in the "original" dataset, but predicts DIED = FALSE cases really badly. In the "removed" dataset it is vice versa. So for the Naive Bayes it seems not possible to get a good prediction rate for both classes.

% \red{
% \begin{itemize}
%     \item Puede ser adecuado porque admite tipos de datos distintos sin ningún problema
%     \item No necesita ningún hiperparámetro
%     \item Puede ser más sensible a haber quitado algunos atributos, pues afecta a la suposición Naive de independencia
%     \item Mostrar la confussion matrix
%     \item Comentar si muestra buenos o malos resutlados
%     \item Comentar diferencias entre cada uno de los dos dataset, y explicar porq qué
%     \item Identificar por qué suponemos que los resultados son buenos o malos.
% \end{itemize}
% }

% \red{
% \begin{itemize}
%     \item Comentar las decisiones que se han tomado (cuidado con los valores por defecto). p.e: Por qué hemos usado distancia Manhatan en vez de Euclidea en knn
%     \item Comentar los hiperparámetros que se han obtenido por crossvalidation. Como no vamos a hablar de cada uno de los modelos (51), hablar de la media de todos ellos, o algo así
%      \item Comentar los resultados que se ha obtenido. Intentar ver por qué ha ido bien o mal, y cosas así
%      \item Poner la confussion matrix
% \end{itemize}
% }
\subsection{K-nearest Neighbours}

K-nearest Neighbours (KNN) will look for similar patients and will predict the the major class among them. This method is quite sensitive to very imbalanced data, since with many more instances in one class the probability of having these instances close to the test cases is very high. Hence, we hope that our resampling method will be suitable for this model, as it only trains it with a balanced dataset of patients.

KNN is also sensitive to non-normalized data. As stated earlier we did some normalization to our continuous variables, but also the used R method should apply further normalization if needed.

As most of our data is boolean, we have chosen to measure with the Manhattan distance instead of the Euclidean one. We expect results wil be better with Manhattan distance since it reflects better de differences among patients.


Looking at the models created with crossvalidation we see that the average number of neighbours considered is \(6.45\) for ``Original'' and \(5.91\) for ``Removed'', which seems like normal values for \(k\). The results obtained using this model are:

\begin{table}[!htbp]
\centering
\caption{KNN Original}
\vspace{0.1cm}
\label{knn-orig}
\begin{tabular}{|l|ll|ll}
\cline{1-3}
Prediction & \multicolumn{2}{l|}{Reference} &  &  \\
           & False          & True          &  &  \\ \cline{2-3}
False      & 58              & 8             &  &  \\
True       & 67              & 16             &  &  \\ \cline{1-3}
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{KNN Removed}
\vspace{0.1cm}
\label{knn-remov}
\begin{tabular}{|l|ll|ll}
\cline{1-3}
Prediction & \multicolumn{2}{l|}{Reference} &  &  \\
           & False          & True          &  &  \\ \cline{2-3}
False      & 73              & 8             &  &  \\
True       & 54              & 14             &  &  \\ \cline{1-3}
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{F2 on KNN}
\vspace{0.1cm}
\label{knn-f2}
\begin{tabular}{|l|l|l|}
\hline
      & Original & Removed \\ \hline
K-nearest neighbours & 0.5123675 & 0.6196944          \\ \hline
\end{tabular}
\end{table}

% \red{Mostrar los resultados y la confussion matrix}
% \red{Comentar los resultados}

The $F2$ score is not very good for none of the datasets, but looking at the confusion matrices it seems that the model achieves very well the task of avoiding false negatives.

\red{Intentar ver por qué ha pasado esto}

% \red{
% \begin{itemize}
%     \item Por qué decidimos usar distancia manhattan en vez de euclidea
%     \item Ver, en promedio, cuantos vecinos se han considerado
%     \item Comentar que nosotros ya hemos normalizado los datos y que hemos puesto de forma adecuada las variables categóricas
%     \item Qué podría afectar a este modelo y cómo nos afectará a nosotros.
%     \item Mostrar los resultado y la confussion matrix
%     \item Comentar los resultados
%     \item Comentar las diferencias entre cada uno de los dos datasets
%     \item Comentar por qué creemos que los resultados han sido buenos o malos
% \end{itemize}
% }

% \red{Si no hacemos nada para desbalancear los datos, hay que usar una k muy pequeña}

% \red{Quizá es recomendable usar algún métozdo para balancear los datos, y probar así otros valores de k}


% \subsection{LDA}
% % \red{Usar LDA para tener 2 centroides, que son los de cada una de las clases.
% % Cuando evaluamos un dato nuevo, le aplicamos la transformación y miramos si queda más cerca de uno o de otro.
% % Así podemos ver la probabilidad de que pertenezca a cada una de las clases.}
% \red{Suponiendo que las varianzas de cada una de las clases son la misma, se
% usa este algoritmo, (que simplifica QLA) para ver la probabilidad de pertenencia
% a una clase}
% \subsection{QDA}
% \subsection{RDA}

\subsection{General Linear Model}

The main problem of using a General Linear Model (GLM) is that is doesn't distinguish between different data types. For this model all the values are real numbers. As most of our variables are categorical, we assume it will not reflect the data very well. In addition, the power of this method relies on the basis function that the user can define, according to the meaning he knows they may have. We have not defined any due to lack of medical knowledge, so this feature won't be used.

% The average \(\lambda\)-value for the models after crossvalidation is \red{value}.

The results of running this model on the dataset are:

\begin{table}[!htbp]
\centering
\caption{GLM Original}
\vspace{0.1cm}
\label{glm-orig}
\begin{tabular}{|l|ll|ll}
\cline{1-3}
Prediction & \multicolumn{2}{l|}{Reference} &  &  \\
           & False          & True          &  &  \\ \cline{2-3}
False      & 80              & 14             &  &  \\
True       & 45              & 10             &  &  \\ \cline{1-3}
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{GLM Removed}
\vspace{0.1cm}
\label{glm-remov}
\begin{tabular}{|l|ll|ll}
\cline{1-3}
Prediction & \multicolumn{2}{l|}{Reference} &  &  \\
           & False          & True          &  &  \\ \cline{2-3}
False      & 86              & 13             &  &  \\
True       & 41              & 9             &  &  \\ \cline{1-3}
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{F2 on GLM}
\vspace{0.1cm}
\label{glm-f2}
\begin{tabular}{|l|l|l|}
\hline
      & Original & Removed \\ \hline
Linear Model & 0.6734007 & 0.7084020       \\ \hline
\end{tabular}
\end{table}

% \red{Poner los resultados y la confussion matrix}
% \red{Comentar los resultados y las diferencias}

The $F2$ score is not as bad as expected. It fact, it performs better than KNN despite the number of false negatives being higher.

% \red{
% \begin{itemize}
%     \item Los datos se considera todos como reales, y no entiende de booleanos o categóricas
%     \item No hemos usado funciones de base especiales, solamente la identidad con cada uno de los atributos.
%     \item Ver en promedio cual ha sido la regularización
%     \item Qué puede afectar a este modelo y cómo nos va a afectar a nosotros
%     \item Mostrar los resultados y la confussion matrix
%     \item Comentar los resultados
%     \item Cometnar las diferencias entre cada uno de los datasets
%     \item Comentar por qué creemos que los resultados han sido buenos o malos
% \end{itemize}
% }






% \red{Mirar el vecino más cercano para precedir}

% \red{Si suponemos que las variables son independientes:
% -Haces naive Bayes para ver la probabilidad de que pertenezca a cada una de las clases
% (habría que estudiar si las variables son independientes)
% - Logistic regression}
